{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "#from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing a regex-formatted string from data text\n",
    "def removeString(data, regex):\n",
    "    return data.str.lower().str.replace(regex.lower(), ' ')\n",
    "\n",
    "# DataFrame version of removeString applied on columns columnsToClean of the dataset\n",
    "#: !!!! does'nt function for the current Python version!!!!!!!!!\n",
    "def cleanDataset(dataset, columnsToClean, regexList):\n",
    "    for column in columnsToClean:\n",
    "        for regex in regexList:\n",
    "            dataset[column] = removeString(dataset[column], regex)\n",
    "    return dataset\n",
    "\n",
    "##### function-like substitute for Type Alias Python Problem!!!\n",
    "def cleanText(text, regexList):\n",
    "    text=text.lower()\n",
    "    for regex in regexList:\n",
    "        text=re.sub(regex,'', text)\n",
    "    return text\n",
    "\n",
    "# Substituting cleanDataset function\n",
    "def cleanDataset2(dataset, columnsToClean, regexList):\n",
    "    for column in columnsToClean:\n",
    "        dataset[column] = dataset[column].apply(lambda x: cleanText(x, regexList)) \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Regex patterns for special strings to remove: phone number, email, http links\n",
    "def get_spec_RegexList():\n",
    "    regexList = []\n",
    "    regexList += ['https?:[^\\]\\n\\r]+']  # https & http\n",
    "    regexList += ['[\\w\\d\\-\\_\\.]+@[\\w\\d\\-\\_\\.]+']  # emails\n",
    "    regexList += ['[0-9][\\-0–90-9 ]+']  # phones\n",
    "    regexList += ['^(?:(?:\\+|00)33[\\s.-]{0,3}(?:\\(0\\)[\\s.-]{0,3})?|0)[1-9](?:(?:[\\s.-]?\\d{2}){4}|\\d{2}(?:[\\s.-]?\\d{3}){2})$'] # french phones\n",
    "    regexList += ['[0-9]']  # numbers\n",
    "    \n",
    "    \n",
    "    \n",
    "    #regexList += ['[^a-zA-z 0-9]+']  # anything that is not a letter\n",
    "    # regexList += ['[\\r\\n]']  # \\r\\n\n",
    "    # regexList += [' [a-zA-Z] ']  # single letters\n",
    "    # regexList += [' [a-zA-Z][a-zA-Z] ']  # two-letter words\n",
    "    #regexList += [\"  \"]  # double spaces\n",
    "\n",
    "    #regexList += ['^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,4})$']\n",
    "    #regexList += ['[\\w\\d\\-\\_\\.]+ @ [\\w\\d\\-\\_\\.]+']\n",
    "    #regexList += ['[^a-zA-Z]']\n",
    "    return regexList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f42d3778baf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### Load dataset from non duplicated file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#filepath=\"PGS-IN.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdfTickets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataFrame shape:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdfTickets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "#### Load dataset from non duplicated file\n",
    "#filepath=\"PGS-IN.csv\"\n",
    "dfTickets=pd.read_csv(filepath,sep=\";\",encoding=\"iso-8859-1\")\n",
    "print('DataFrame shape:',dfTickets.shape)\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of 'Incident description' formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#  ############### Dictionary of text Formats ###################\n",
    "\n",
    "# Email_like format\n",
    "#regex_Email_like = r\"\\* note: please check the attachments tab for complete \"\n",
    "regex_Email_like =r'(objet :|subject:)'\n",
    "# Impulse_like format\n",
    "regex_Impulse_like =r\"ticket description =*\"\n",
    "# Incident_Like format\n",
    "regex_Incident_like =r\"descriptif de l\\'incident :\"\n",
    "# Detail_Incident_Like_Format\n",
    "regex_Detail_Incident_Like=r\"détail de l\\'incident ou de la demande:\"\n",
    "# Description_Like format\n",
    "#regex_Description_like =r\"descript(.*?):(.*?(:|$))\"\n",
    "#regex_Description_like =r\"sdescript(.*?):(.*?(:|$))\"\n",
    "# Save to format dictionary\n",
    "#Dict_Format={'Email_like':regex_Email_like,'Impulse_like':regex_Impulse_like,'Incident_like':regex_Incident_like,'Description_like':regex_Description_like}\n",
    "Dict_Format={'Email_like':regex_Email_like,'Impulse_like':regex_Impulse_like,'Incident_like':regex_Incident_like,'Detail_Incident_Like':regex_Detail_Incident_Like}\n",
    "\n",
    "\n",
    "\n",
    "# Array of different text format regex\n",
    "Array_Format=list(Dict_Format.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate format (recorded in Dict_Format) for each text field\n",
    "def format_type(text,Dict_Format):\n",
    "    text_format='free' # return free format by default\n",
    "    for (k,v) in Dict_Format.items():\n",
    "        if bool(re.search(v, text)):\n",
    "            text_format=k\n",
    "            break\n",
    "    return text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Adding new field for text format\n",
    "dfTickets_format=dfTickets.copy()\n",
    "dfTickets_format['Incident_Description_format']=dfTickets_format['IN - Incident Description'].str.lower().apply(lambda x: format_type(x,Dict_Format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets_format['IN - Incident Description'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets_format[['Incident_Description_format','IN - Incident Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Histogram of processed formats  ###################\n",
    "format_count=dfTickets_format.groupby('Incident_Description_format')['ID Incident'].count()\n",
    "print(format_count)\n",
    "format_count.plot(kind='bar',figsize=(6,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing of 'Incident description'  field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Dictionary of main key regex (significative field for each format) for parsing############\n",
    "\n",
    "Dict_regex_main_parse={'Email_like':r'(objet :|subject:)','Impulse_like':r'ticket description =*','Incident_like':r'descriptif de l\\'incident :','Detail_Incident_Like':r'détail de l\\'incident ou de la demande:'}\n",
    "\n",
    "\n",
    "######### Dictionary of Format Parsing ############\n",
    "regex_main_parse={'Email_like':r'(objet :|subject:)','Impulse_like':r'ticket description =*','Incident_like':r'descriptif de l\\'incident :'}\n",
    "# pat for Incident_like format\n",
    "pat_Incident=r'(descriptif de l\\'incident :|actions réalisées:|depuis quand :|message d\\'erreur|information requises :|nom du matérie l:|localisation :|téléphone \\(complet\\) :|os :)'\n",
    "# pat for Impulse_like format\n",
    "pat_Impulse=r'(ticket description =*|further information from impulse=* end user:|caller:|contact type:|impact:|urgency:|impulse owning group:|impulse category level 1:|impulse category level 2:|impulse category level 3:| created by:|created_at :|impacted_environment :|end user\\'s location :)'\n",
    "# pat for E-mail_like format\n",
    "pat_Email=r'(objet :|subject:|note:|de :|from:|envoyé :|sent:|à :|to:|cc :|phone: &#43;|fax : &#43;|email:|message d\\'origine-----|tél.       &#43;|e-mail :)'\n",
    "# pat for Detail_Incident_Like format\n",
    "pat_Detail_Incident=r'(détail de l\\'incident ou de la demande:|ma du poste :|manipulations déjà effectuées:)'\n",
    "\n",
    "Dict_format_parsing={'Email_like':pat_Email,'Impulse_like':pat_Impulse,'Incident_like':pat_Incident,'Detail_Incident_Like':pat_Detail_Incident}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the main key to be sporsed\n",
    "def extract_key(regex,dict_parse):\n",
    "        return re.search(regex, \"\".join(list(dict_parse.keys()))).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally parse the portion of text (corresponding to the extracted main key)\n",
    "def parse_text(text,Dict_Format,Dict_format_parsing,Dict_regex_main_parse):\n",
    "#    print(text)\n",
    "    format_text=format_type(text,Dict_Format)\n",
    "    if format_text!='free':\n",
    "        Dict_parse=dict(zip(*[iter(re.split(Dict_format_parsing[format_text], text, re.MULTILINE)[1:])]*2))\n",
    "        return Dict_parse[extract_key(Dict_regex_main_parse[format_text], Dict_parse)]\n",
    "    else:\n",
    "        return text # do nothing for free-formatted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=i+1\n",
    "text=dfTickets_format['IN - Incident Description'][i].lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_text(text,Dict_Format,Dict_format_parsing,Dict_regex_main_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_format_parsing['Detail_Incident_Like']\n",
    "#dict(zip(*[iter(re.split(Dict_format_parsing['Detail_Incident_Like'], text, re.MULTILINE)[1:])]*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict_parse=dict(zip(*[iter(re.split(Dict_format_parsing[format_text], text, re.MULTILINE)[1:])]*2))\n",
    "Dict_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new field 'Parsed_Incident_Description'\n",
    "dfTickets_parse=dfTickets_format.copy()\n",
    "dfTickets_parse['Parsed_Incident_Description']=dfTickets_parse['IN - Incident Description'].str.lower().apply(lambda x: parse_text(x,Dict_Format,Dict_format_parsing,Dict_regex_main_parse) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets_parse[['IN - Title','IN - Incident Description','Parsed_Incident_Description','Incident_Description_format']].sample(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First round cleaning of text fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## First Step of Cleaning ##############\n",
    "# Cleaning text fields of DF: dfTickets_parse, the fields are mentioned in columnsToClean list  \n",
    "columnsToClean=['Parsed_Incident_Description','IN - Title']\n",
    "dfTickets_parse_clean_1=dfTickets_parse.copy()\n",
    "cleanDataset2(dfTickets_parse_clean_1, columnsToClean,  get_spec_RegexList())\n",
    "# Display cleaned fields\n",
    "dfTickets_parse_clean_1[['IN - Title','IN - Incident Description','Parsed_Incident_Description','Incident_Description_format']].sample(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets_parse_clean_1['Parsed_Incident_Description'][5221]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting language of Parsed Incident Description field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for detecting english or french text based on frequency of common stop words for each language. May be not efficient for short texts!!\n",
    "\n",
    "def detect_english_french(text):\n",
    "    stopwords_fileids_custom=['english','french']\n",
    "    ratios = {}\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "#    for lang in stopwords.fileids():\n",
    "    for lang in stopwords_fileids_custom:\n",
    "        stopwords_set = set(stopwords.words(lang))\n",
    "        words_set = set(words)\n",
    "        common_words = words_set.intersection(stopwords_set)\n",
    "        ratios[lang] = len(common_words)\n",
    "    \n",
    "    if ratios['english']>ratios['french']:\n",
    "        lang_text='english'\n",
    "    else:\n",
    "        lang_text='french'       \n",
    "\n",
    "    return lang_text\n",
    "\n",
    "# driver to mix detect function from langdetect package and  detect_english_french function\n",
    "def detect_lang(text):\n",
    "    print('--------\\n')\n",
    "    print(text)\n",
    "    \n",
    "    if len(text)>0 and len(text)<6:\n",
    "        result=detect(text)\n",
    "    else:\n",
    "        result=detect_english_french(text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new field 'Detected_language' to cleaned DataFrame\n",
    "#dfTickets_parse_clean_1['Detected_language']=dfTickets_parse_clean_1['Parsed_Incident_Description'].apply(detect_lang)\n",
    "dfTickets_parse_clean_1['Detected_language']=dfTickets_parse_clean_1['Parsed_Incident_Description'].apply(detect_english_french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new field\n",
    "dfTickets_parse_clean_1[['IN - Title','Parsed_Incident_Description','Detected_language']].sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Histogram of detected languages among english or french  ###################\n",
    "format_count=dfTickets_parse_clean_1.groupby('Detected_language')['ID Incident'].count()\n",
    "print(format_count)\n",
    "format_count.plot(kind='bar',figsize=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTickets_parse_clean_1['Parsed_Incident_Description'][8673]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='bonjour les amis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_english_french(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
